# =============================================================================
# Cassandra to YugabyteDB Migration Configuration
# =============================================================================

# =============================================================================
# Cassandra Connection Settings
# =============================================================================
cassandra.host=localhost
cassandra.port=9043
cassandra.localDC=datacenter1
cassandra.username=
cassandra.password=

# Cassandra Read Settings (Optimized for 10K+ IOPS)
cassandra.readTimeoutMs=120000
cassandra.fetchSizeInRows=10000
cassandra.consistencyLevel=LOCAL_ONE
cassandra.retryCount=3
# Split size optimization (can be auto-determined at runtime)
# Set cassandra.inputSplitSizeMb.override to force a specific value
# Set cassandra.inputSplitSizeMb.autoDetermine=false to disable auto-determination
cassandra.inputSplitSizeMb=256
cassandra.inputSplitSizeMb.autoDetermine=true
# cassandra.inputSplitSizeMb.override=512
cassandra.concurrentReads=2048
cassandra.readsPerSec=0

# Cassandra Spark Connector Advanced Settings (CDM-style)
# Connection pool settings
cassandra.connection.localConnectionsPerExecutor=4
cassandra.connection.remoteConnectionsPerExecutor=1
cassandra.connection.timeoutMs=60000
cassandra.connection.keepAliveMs=30000
cassandra.connection.reconnectionDelayMs.min=1000
cassandra.connection.reconnectionDelayMs.max=60000
cassandra.connection.maxRequestsPerConnection.local=32768
cassandra.connection.maxRequestsPerConnection.remote=2000
cassandra.connection.factory=com.datastax.spark.connector.cql.DefaultConnectionFactory

# SSL/TLS Settings (optional - uncomment if needed)
# cassandra.connection.ssl.enabled=false
# cassandra.connection.ssl.trustStore.path=
# cassandra.connection.ssl.trustStore.password=
# cassandra.connection.ssl.keyStore.path=
# cassandra.connection.ssl.keyStore.password=

# =============================================================================
# YugabyteDB Connection Settings
# =============================================================================
yugabyte.host=localhost
yugabyte.port=5433
yugabyte.database=transaction_datastore
yugabyte.username=yugabyte
yugabyte.password=yugabyte

# YugabyteDB Connection Pool Settings
yugabyte.maxPoolSize=8
yugabyte.minIdle=2
yugabyte.connectionTimeout=30000
yugabyte.idleTimeout=300000
yugabyte.maxLifetime=1800000

# YugabyteDB JDBC Parameters (COPY-optimized)
yugabyte.loadBalanceHosts=true
yugabyte.reWriteBatchedInserts=true
yugabyte.tcpKeepAlive=true
yugabyte.binaryTransfer=false
yugabyte.socketTimeout=0
yugabyte.loginTimeout=10
# Topology keys for stretch clusters (optional)
# Format: region1.zone1,region2.zone1
# yugabyte.topologyKeys=

# YugabyteDB COPY Settings (Optimized for 10K+ IOPS)
yugabyte.copyBufferSize=100000
yugabyte.copyFlushEvery=50000
yugabyte.csvDelimiter=,
yugabyte.csvNull=
yugabyte.csvQuote="
yugabyte.csvEscape="

# YugabyteDB Transaction Settings
yugabyte.isolationLevel=READ_COMMITTED
yugabyte.autoCommit=false

# =============================================================================
# Spark Job Configuration (Optimized for 10K+ IOPS)
# =============================================================================
# Optimized for local machine - balance parallelism vs overhead
spark.executor.instances=4
spark.executor.cores=4
spark.executor.memory=8g
spark.executor.memoryOverhead=2048m
spark.driver.memory=4g
# Optimal parallelism - not too many small partitions
spark.default.parallelism=16
spark.sql.shuffle.partitions=16
# Optimized memory settings for COPY workloads
spark.memory.fraction=0.8
spark.memory.storageFraction=0.2
spark.task.maxFailures=10
spark.stage.maxConsecutiveAttempts=4
spark.network.timeout=800s
spark.serializer=org.apache.spark.serializer.KryoSerializer
# Disable dynamic allocation for consistent performance
spark.dynamicAllocation.enabled=false
spark.dynamicAllocation.minExecutors=8
spark.dynamicAllocation.maxExecutors=16
spark.dynamicAllocation.initialExecutors=8
# Additional performance optimizations
spark.locality.wait=0s
spark.executor.heartbeatInterval=60s
spark.sql.adaptive.enabled=true
spark.sql.adaptive.coalescePartitions.enabled=true

# =============================================================================
# Migration Settings
# =============================================================================
migration.jobId=migration-job-${timestamp}
migration.checkpoint.enabled=true
migration.checkpoint.table=migration_checkpoint
migration.checkpoint.interval=50000
migration.validation.enabled=true
migration.validation.sampleSize=1000

# =============================================================================
# Table Configuration
# =============================================================================
# Source Cassandra keyspace and table
table.source.keyspace=transaction_datastore
table.source.table=dda_pstd_fincl_txn_cnsmr_by_accntnbr

# Target YugabyteDB schema and table
table.target.schema=public
table.target.table=dda_pstd_fincl_txn_cnsmr_by_accntnbr

# Column mapping (optional - leave empty if column names match)
# Format: cassandra_column=yugabyte_column
# Example: table.columnMapping.cass_col1=yb_col1

# Type mapping (optional - leave empty for default mapping)
# Format: cassandra_type=yugabyte_type

# Primary key columns (optional - will be discovered from schema if not specified)
# Format: table.primaryKey=col1,col2,col3

# Enable validation for this table
table.validate=true

